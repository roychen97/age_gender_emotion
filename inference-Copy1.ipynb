{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "#from PIL import Image\n",
    "import cv2 as cv\n",
    "import imutils\n",
    "model_file1 = \"./trans_model/tf_model.pb\"\n",
    "model_file2 = \"./trans_model/frozen_onet_model.pb\"\n",
    "#model_path = \"./frozentensorflowModel.pb\"\n",
    "#pbtxt_path = \"/workspace/testbuildmodel/glassdemo/dataset/glassdemo.pbtxt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testimg = \"/workspace/testbuildmodel/glassdemo/dataset/Folder/lighter_video/1/out155.jpeg\"\n",
    "testvideo= \"/workspace/testbuildmodel/glassdemo/dataset/Folder/lighter_video/1/lighter_1.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncap = cv.VideoCapture(0)\\nwhile(True):\\n    print(\"fefe0\")\\n  # \\xe5\\xbe\\x9e\\xe6\\x94\\x9d\\xe5\\xbd\\xb1\\xe6\\xa9\\x9f\\xe6\\x93\\xb7\\xe5\\x8f\\x96\\xe4\\xb8\\x80\\xe5\\xbc\\xb5\\xe5\\xbd\\xb1\\xe5\\x83\\x8f\\n    ret, frame = cap.read()\\n    print(\"fefe1\")\\n  # \\xe9\\xa1\\xaf\\xe7\\xa4\\xba\\xe5\\x9c\\x96\\xe7\\x89\\x87\\n    cv.imshow(\\'frame\\', frame)\\n    print(\"fefe2\")\\n  # \\xe8\\x8b\\xa5\\xe6\\x8c\\x89\\xe4\\xb8\\x8b q \\xe9\\x8d\\xb5\\xe5\\x89\\x87\\xe9\\x9b\\xa2\\xe9\\x96\\x8b\\xe8\\xbf\\xb4\\xe5\\x9c\\x88\\n    if cv.waitKey(1) & 0xFF == ord(\\'q\\'):\\n        break\\n\\n# \\xe9\\x87\\x8b\\xe6\\x94\\xbe\\xe6\\x94\\x9d\\xe5\\xbd\\xb1\\xe6\\xa9\\x9f\\ncap.release()\\n\\n# \\xe9\\x97\\x9c\\xe9\\x96\\x89\\xe6\\x89\\x80\\xe6\\x9c\\x89 OpenCV \\xe8\\xa6\\x96\\xe7\\xaa\\x97\\ncv.destroyAllWindows()\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "cap = cv.VideoCapture(0)\n",
    "while(True):\n",
    "    print(\"fefe0\")\n",
    "  # 從攝影機擷取一張影像\n",
    "    ret, frame = cap.read()\n",
    "    print(\"fefe1\")\n",
    "  # 顯示圖片\n",
    "    cv.imshow('frame', frame)\n",
    "    print(\"fefe2\")\n",
    "  # 若按下 q 鍵則離開迴圈\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 釋放攝影機\n",
    "cap.release()\n",
    "\n",
    "# 關閉所有 OpenCV 視窗\n",
    "cv.destroyAllWindows()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "model_file2 = \"./trans_model/model_80.pb\"\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "with tf.Session() as sess:\n",
    "    model_filename =model_file2\n",
    "    with gfile.FastGFile(model_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "        g_in = tf.import_graph_def(graph_def)\n",
    "LOGDIR='./logs/tests/1/'\n",
    "train_writer = tf.summary.FileWriter(LOGDIR)\n",
    "train_writer.add_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith tf.gfile.FastGFile(model_path, 'rb') as f:\\n    graph_def = tf.GraphDef()\\n    graph_def.ParseFromString(f.read())\\n\\nwith tf.Session() as sess:\\n    # Restore session\\n    sess.graph.as_default()\\n    tf.import_graph_def(graph_def, name='')\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with tf.gfile.FastGFile(model_path, 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore session\n",
    "    sess.graph.as_default()\n",
    "    tf.import_graph_def(graph_def, name='')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(model_file):\n",
    "    graph = tf.Graph()\n",
    "    graph_def = tf.GraphDef()\n",
    "\n",
    "    with open(model_file, \"rb\") as f:\n",
    "        graph_def.ParseFromString(f.read())\n",
    "    with graph.as_default():\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_graph1 = load_graph(model_file1)\n",
    "sess1=tf.Session(graph=classify_graph1)\n",
    "\n",
    "classify_graph2 = load_graph(model_file2)\n",
    "sess2=tf.Session(graph=classify_graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for op in tf.get_default_graph().get_operations():\n",
    "    #print(str(op.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess2.graph.get_operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time\\n\\nwith tf.Session() as sess:\\n    # Restore session\\n\\n    #alltensor=sess.graph.get_operations()\\n    #print(alltensor)\\n    cap = cv.VideoCapture(0)\\n    cap.set(cv.CAP_PROP_FRAME_WIDTH, 1280);\\n    cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720);\\n    while(True):\\n        \\n       \\n        ret, img = cap.read()\\n        # Read and preprocess an image.\\n        #img = cv.imread(testimg)\\n        rows = img.shape[0]\\n        cols = img.shape[1]\\n        img2=img.copy()\\n        inp = cv.resize(img2, (300, 300))\\n        #inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\\n\\n        # Run the model\\n        start_time = time.time()\\n        print(\"FFFFF\")\\n        out = sess.run([sess.graph.get_tensor_by_name(\\'claasf1_1/dense_1/BiasAdd:0\\'),],\\n                       feed_dict={\\'Reshape:0\\': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\\n        print(\"FFFFF\")\\n        elapsed_time = time.time() - start_time\\n        print(elapsed_time)\\n        predctedArray=out[0][0].astype(int)\\n        \\n        # Visualize detected bounding boxes.\\n\\n        x1=max(0, min(predctedArray[2]-179, 319))\\n        if(x1!=319):\\n            y1=max(0, min(predctedArray[2]-x1, 179))\\n        else:\\n            y1=max(0, min(179-(predctedArray[2]-319-179), 179))\\n\\n        x2=max(0, min(predctedArray[3]-179, 319))\\n        if(x2!=319):\\n            y2=max(0, min(predctedArray[3]-x1, 179))\\n        else:\\n            y2=max(0, min(179-(predctedArray[2]-319-179), 179))  \\n        #print(rows)\\n        #print(cols)\\n        img=cv.resize(img, (320, 180))\\n        cv.line(img, (predctedArray[0],predctedArray[1]), ( x1,y1), (0,0,255), thickness=1, lineType=8,  shift=0)\\n        cv.line(img, (predctedArray[0],predctedArray[1]), ( x2,y2),(0,0,255), thickness=1, lineType=8,  shift=0)\\n\\n        #cv.putText(img, text, (10, 80), cv.FONT_HERSHEY_PLAIN,1, (0, 255, 255), 1, cv.LINE_AA)\\n\\n                #print(classId, \"-->\", score, x, y)\\n        cv.imshow(\\'frame\\', img)\\n\\n        if cv.waitKey(1) & 0xFF == ord(\\'q\\'):\\n            break\\ncap.release()\\n\\n\\ncv.destroyAllWindows()\\n#cv.imshow(\\'SHOWfg\\', imutils.resize(img, width=800))\\n#cv.waitKey()\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Restore session\n",
    "\n",
    "    #alltensor=sess.graph.get_operations()\n",
    "    #print(alltensor)\n",
    "    cap = cv.VideoCapture(0)\n",
    "    cap.set(cv.CAP_PROP_FRAME_WIDTH, 1280);\n",
    "    cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720);\n",
    "    while(True):\n",
    "        \n",
    "       \n",
    "        ret, img = cap.read()\n",
    "        # Read and preprocess an image.\n",
    "        #img = cv.imread(testimg)\n",
    "        rows = img.shape[0]\n",
    "        cols = img.shape[1]\n",
    "        img2=img.copy()\n",
    "        inp = cv.resize(img2, (300, 300))\n",
    "        #inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "\n",
    "        # Run the model\n",
    "        start_time = time.time()\n",
    "        print(\"FFFFF\")\n",
    "        out = sess.run([sess.graph.get_tensor_by_name('claasf1_1/dense_1/BiasAdd:0'),],\n",
    "                       feed_dict={'Reshape:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "        print(\"FFFFF\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(elapsed_time)\n",
    "        predctedArray=out[0][0].astype(int)\n",
    "        \n",
    "        # Visualize detected bounding boxes.\n",
    "\n",
    "        x1=max(0, min(predctedArray[2]-179, 319))\n",
    "        if(x1!=319):\n",
    "            y1=max(0, min(predctedArray[2]-x1, 179))\n",
    "        else:\n",
    "            y1=max(0, min(179-(predctedArray[2]-319-179), 179))\n",
    "\n",
    "        x2=max(0, min(predctedArray[3]-179, 319))\n",
    "        if(x2!=319):\n",
    "            y2=max(0, min(predctedArray[3]-x1, 179))\n",
    "        else:\n",
    "            y2=max(0, min(179-(predctedArray[2]-319-179), 179))  \n",
    "        #print(rows)\n",
    "        #print(cols)\n",
    "        img=cv.resize(img, (320, 180))\n",
    "        cv.line(img, (predctedArray[0],predctedArray[1]), ( x1,y1), (0,0,255), thickness=1, lineType=8,  shift=0)\n",
    "        cv.line(img, (predctedArray[0],predctedArray[1]), ( x2,y2),(0,0,255), thickness=1, lineType=8,  shift=0)\n",
    "\n",
    "        #cv.putText(img, text, (10, 80), cv.FONT_HERSHEY_PLAIN,1, (0, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "                #print(classId, \"-->\", score, x, y)\n",
    "        cv.imshow('frame', img)\n",
    "\n",
    "        if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "#cv.imshow('SHOWfg', imutils.resize(img, width=800))\n",
    "#cv.waitKey()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_1_2:0\n",
    "#dense_1_2/BiasAdd:0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "    # Restore session\n",
    "\n",
    "    #alltensor=sess.graph.get_operations()\n",
    "    #print(alltensor)\n",
    "cap = cv.VideoCapture(0)\n",
    "cap.set(cv.CAP_PROP_FRAME_WIDTH, 1280);\n",
    "cap.set(cv.CAP_PROP_FRAME_HEIGHT, 720);\n",
    "while(True):\n",
    "\n",
    "    print(cap)\n",
    "    ret, img = cap.read()\n",
    "    # Read and preprocess an image.\n",
    "    #img = cv.imread(testimg)\n",
    "    rows = img.shape[0]\n",
    "    cols = img.shape[1]\n",
    "    img2=img.copy()\n",
    "    midpint=cols/2\n",
    "    print (img2.shape)\n",
    "    img2 = img2[:, midpint-rows/2:midpint+rows/2,:]\n",
    "    print(img2.shape)\n",
    "    inp = cv.resize(img2, (100, 100))\n",
    "    inp2 = cv.resize(img2, (48, 48))\n",
    "    #inp2 = inp2[:, :, [2, 1, 0]]\n",
    "    inp2=(inp2 - 127.5) * 0.0078125\n",
    "    #print(in2p)\n",
    "    #inp = inp[:, :, [2, 1, 0]]  # BGR2RGB\n",
    "\n",
    "    # Run the model\n",
    "    start_time = time.time()\n",
    "\n",
    "    out2 = sess2.run([sess2.graph.get_tensor_by_name('import/onet/conv6-2/onet/conv6-2:0'),sess2.graph.get_tensor_by_name('import/onet/conv6-3/onet/conv6-3:0')],\n",
    "       feed_dict={'import/Placeholder:0': inp2.reshape(1, inp2.shape[0], inp2.shape[1], 3)})\n",
    "    elapsed_time1 = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    out1 = sess1.run([sess1.graph.get_tensor_by_name('import/dense_1/BiasAdd:0'),],\n",
    "                   feed_dict={'import/input_1:0': inp.reshape(1, inp.shape[0], inp.shape[1], 3)})\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    elapsed_time2 = time.time() - start_time\n",
    "    print(elapsed_time1)\n",
    "    print(elapsed_time2)\n",
    "    predctedArray=out1[0][0]\n",
    "    print('############')\n",
    "    predctedArray2=out2[1]\n",
    "    predctedArray2=(predctedArray2+1)/2\n",
    "    predctedArray2=predctedArray2[0]\n",
    "    print(predctedArray2.shape)\n",
    "    print(predctedArray2)\n",
    "    print('############')\n",
    "    # Visualize detected bounding boxes.\n",
    "\n",
    "\n",
    "    #print(predctedArray)\n",
    "    #print(rows)\n",
    "    #print(cols)\n",
    "    img=cv.resize(img2, (300, 300))\n",
    "    cv.ellipse(img, (int(predctedArray[0]*300),int(predctedArray[1]*300)), (20, 20), 0, 0, 0, (205, 0, 255), 10)\n",
    "    cv.ellipse(img, (int(predctedArray[2]*300),int(predctedArray[3]*300)), (20, 20), 0, 0, 0, (205, 0, 255), 10)\n",
    "    cv.ellipse(img, (int(predctedArray[4]*300),int(predctedArray[5]*300)), (20, 20), 0, 0, 0, (205, 0, 255), 10)\n",
    "    cv.ellipse(img, (int(predctedArray[6]*300),int(predctedArray[7]*300)), (20, 20), 0, 0, 0, (205, 0, 255), 10)\n",
    "    cv.ellipse(img, (int(predctedArray[8]*300),int(predctedArray[9]*300)), (20, 20), 0, 0, 0, (205, 0, 255), 10)\n",
    "   \n",
    "    cv.ellipse(img, (int(predctedArray2[0]*300),int(predctedArray2[1]*300)), (20, 20), 0, 0, 0, (205, 0, 0), 10)\n",
    "    cv.ellipse(img, (int(predctedArray2[2]*300),int(predctedArray2[3]*300)), (20, 20), 0, 0, 0, (205, 0, 0), 10)\n",
    "    cv.ellipse(img, (int(predctedArray2[4]*300),int(predctedArray2[5]*300)), (20, 20), 0, 0, 0, (205, 0, 0), 10)\n",
    "    cv.ellipse(img, (int(predctedArray2[6]*300),int(predctedArray2[7]*300)), (20, 20), 0, 0, 0, (205, 0, 0), 10)\n",
    "    cv.ellipse(img, (int(predctedArray2[8]*300),int(predctedArray2[9]*300)), (20, 20), 0, 0, 0, (205, 0, 0), 10)\n",
    "    #cv.putText(img, text, (10, 80), cv.FONT_HERSHEY_PLAIN,1, (0, 255, 255), 1, cv.LINE_AA)\n",
    "\n",
    "            #print(classId, \"-->\", score, x, y)\n",
    "    cv.imshow('frame', img)\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "\n",
    "\n",
    "cv.destroyAllWindows()\n",
    "#cv.imshow('SHOWfg', imutils.resize(img, width=800))\n",
    "#cv.waitKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
